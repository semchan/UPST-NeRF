
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>


<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis</title>
    <meta property="og:description" content="Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="http://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://kangxue.org/">Kangxue Yin</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://mingyuliu.net/">Ming-Yu Liu</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-3 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-3 text-center"><sup>2</sup>University of Toronto</div>
            <div class="col-3 text-center"><sup>3</sup>Vector Institute</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>NeurIPS 2021</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/dmtet.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="assets/suppl.zip">
                <span class="material-icons"> description </span> 
                 Supplement
            </a>
            <a class="supp-btn" href="assets/dmtet-bib.txt">
                <span class="material-icons"> description </span> 
                  BibTeX
            </a>
            <a class="supp-btn" href="https://papertalk.org/papertalks/35710">
                <span class="material-icons"> description </span> 
                  Video
            </a>
        </div></div>
    </div>

    <section id="teaser">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        <p class="caption">We introduce <strong>DMTet, a deep 3D conditional generative model</strong> that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels or noisy point cloud.
        </p>
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/twitter.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
    </section>

    <section id="news">
        <h2>News</h2>
        <hr>
        <div class="row">
            <div><span class="material-icons"> event </span> [Dec 2021] Check out our <a href="https://github.com/NVIDIAGameWorks/kaolin/blob/master/examples/tutorial/dmtet_tutorial.ipynb">tutorial</a> that demonstrates using DMTet for 3D reconstruction! Full code coming soon. </div>
        </div>
    </section>


    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>

        <p>We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.
            </p>
    </section>

    <hr>
    <section id="teaser-videos">
        <div class="flex-row">
        <figure style="width: 50%;">
            <video class="centered" width="90%" controls muted loop autoplay>
                <source src="assets/mtri.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
            <div style="width: 50%;">
                <br><br>
                <p> <strong>DMTet</strong> predicts the underlying surface parameterized by an implicit function encoded via <strong>a deformable tetrahedral grid</strong>. The underlying surface is converted into an explicit mesh with a Marching Tetrahedra (MT) algorithm, which we show is differentiable. Therefore, DMTet can jointly optimize the surface geometry and topology using <strong>losses defined expliciitly on the surface mesh</strong>.</p> 

                <p>Here we demonstrate this with a 2D example, where the loss is defined as the distance between extracted surface (shown in <span style="color: red">red</span>) with ground truth point cloud (shown in <span style="color: purple">purple</span>). </p>
            </div>
        </div>
    </section>






    <section id="results">
        <h2>3D Shape Synthesis from Coarse Voxels</h2>
        <hr>
        <figure style="width: 100%;">
            <a href="assets/animal_qual.png">
                <img width="100%" src="assets/animal_qual.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
                Qualitative results on 3D Shape Synthesis from Coarse Voxels. Comparing with all baselines, our method (fifth column) reconstructs shapes with much higher quality. Adding GAN (highlighted in <span style="color: orange">orange</span>) further improves the realism of the generated shape. We also show the retrieved shapes from the training set in the second last column.
            </p>
        </figure>
        
        <br/>
        <hr>
        <section id="teaser-videos">
            <div class="flex-row">
            <figure style="width: 60%;">
                <a href="assets/minecraft.png">
                    <img width="100%" src="assets/minecraft.png">
                </a>
            </figure>
                <div style="width: 40%;">
                    <br><br>
                    <p> DMTet generalizes to <strong>human-created</strong> low-resolution voxels collected online. Despite the fact that these human-created shapes (in <span style="color: gold">yellow</span>) have noticeable differences with our coarse voxels used in training, e.g., different ratios of body parts compared with our training shapes (larger head, thinner legs, longer necks), our model faithfully generates high-quality 3D details (in <span style="color: blue">blue</span>) conditioned on each coarse voxel â€“ an exciting result.</p>
                </div>
            </div>
        </section>

    </section>
    
    
    
    
    
    
    <section id="results">
        <h2> Point Cloud 3D Reconstruction</h2>
        <hr>
        <figure style="width: 100%;">
            <a href="assets/pcd_qual.png">
                <img width="100%" src="assets/pcd_qual.png">
            </a>
            <p class="caption"> Qualitative results on 3D Reconstruction from Point Clouds: Our model reconstructs shapes with more geometric details compared to baselines using different representations - voxels, deforming a mesh with a fixed template, deforming a mesh generated from a volumetric representation, tetrahedral mesh, and implicit functions.
            </p>
        </figure>
        
        <figure style="width: 100%;">
            <a href="assets/pcd_quan.png">
                <img width="100%" src="assets/pcd_quan.png">
            </a>
            <p class="caption"> Quantitative Results on <strong>Point Cloud Reconstruction</strong> (Chamfer L1). Our model outperforms ConvOnet, the SOTA implicit approach, across all object categories even running at a low grid resolution (in <span style="color: gold;">yellow</span>). Our model run significantly faster at inference time and produces an explicit mesh as output, making it suitable for interactive graphic application. <br> With volume and surface subdivision (in <span style="color: blue">blue</span>) we can further boost the reconstruction quality. In this case, we apply volume subdivision once which double the grid resolution. The runtime is only doubled instead of growing cubically as resolution increases.


            </p>
        </figure>
        <figure style="width: 100%;">
            <a href="assets/oracle.png">
                <img width="100%" src="assets/oracle.png">
            </a>
            <p class="caption"> 
                <p> We demonstrate the effect of learning on surface by comparing with the oracle performance of MT/MC evaluated on ShapeNet Chairs. Without deforming the grid, DMTET outperforms the oracle performance of MT by a large margin when querying the same number of points, although DMTET predicts the surface from noisy point cloud. This demonstrates that directly optimizing the reconstructed surface can mitigate the discretization errors imposed by MT to a large extent.
                </p>
        </figure>

    </section>
    
    



    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
        @inproceedings{shen2021dmtet,
        title = {Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis},
        author = {Tianchang Shen and Jun Gao and Kangxue Yin and Ming-Yu Liu and Sanja Fidler},
        year = {2021},
        booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
        }

</code></pre>
    </section>

<br />
    
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/dmtet.pdf"><img class="screenshot" src="assets/paper_preview.png"></a>
            </div>
            <div style="width: 50%">
                <p><b>Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis</b></p>
                <p>Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, Sanja Fidler</p>

                <div><span class="material-icons"> description </span><a href="assets/dmtet.pdf"> Paper camera-ready</a></div>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2111.04276"> arXiv version</a></div>
                <div><span class="material-icons"> description </span><a href="assets/suppl.zip"> Supplement</a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/dmtet-bib.txt"> BibTeX</a></div>
            </div>
        </div>
    </section>

</div>
</body>
</html>